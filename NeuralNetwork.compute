

// Declare the compute shader kernel function
#pragma kernel CSMain
#pragma kernel Train
#pragma kernel Backpropagation
#pragma kernel Inference

RWStructuredBuffer<int> InputCount;
RWStructuredBuffer<int[]> HiddenLayerStructure;
RWStructuredBuffer<int> HiddenLayerCount;
RWStructuredBuffer<int> OututCount;
RWStructuredBuffer<half> LearningRate;

RWStructuredBuffer<half[]> InputBuffer;
RWStructuredBuffer<half[]> MutationRate;

// Define the activation function for the neurons
float sigmoid(float x) {
    return 1.0 / (1.0 + exp(-x));
}

// Define the derivative of the activation function for backpropagation
float sigmoid_derivative(float x) {
    float s = sigmoid(x);
    return s * (1.0 - s);
}

// Define the neuron structure
struct Neuron {
    float bias;
    float weights[HiddenLayerStructure];
};

// Define the network structure
struct Network {
    Neuron input_layer[InputCount];
    Neuron hidden_layer[HiddenLayerCount][];
    Neuron output_layer[OututCount];
};

// Declare the compute shader output buffer
RWStructuredBuffer<Network> infered : register(u0);
RWStructuredBuffer<Network> backpropagated : register(u1);


// Define the backpropagation algorithm
[numthreads(8, 8, 1)] // The number of threads to execute in each thread group
void Backpropagation(uint3 id : SV_DispatchThreadID, Network network, float inputs[], float targets[], out float error) {
    // Forward pass
    float hidden_activations[HiddenLayerStructure];
    float output_activations[OututCount];

    // Calculate activations for the input layer
    for (int i = 0; i < HiddenLayerCount[id.x]; i++) {
        float activation = 0.0;

        for (int j = 0; j < InputCount[id.x];j++) {
            activation += inputs[j] * network.input_layer[j].weights[i];
        }
        
        activation += network.hidden_layer[0][i].bias;
        hidden_activations[i] = sigmoid(activation);
    }

    // Calculate activations for the hidden layers
    for (int i = 0; i < HiddenLayerCount[id.x]; i++)
    {
        float activation = 0.0;
        for (int j = 0; j < HiddenLayerStructure[id.x][i]; j++)
        {
            for (int k = 0; k < HiddenLayerStructure[id.x][i - 1]; k++)
            {
                activation += hidden_activations[j] * network.hidden_layer[i][j].weights[i];
            }
            
            activation += network.hidden_layer[i][j].bias;
            hidden_activations[i] = sigmoid(activation);
        };
    } 
    
    // Calculate activations for the output layer
    for (int i = 0; i < OututCount[id.x]; i++) {
        float activation = 0.0;
        for (int j = 0; j < HiddenLayerStructure[id.x][HiddenLayerCount[id.x] - 1]; j++) {
            activation += hidden_activations[j] * network.hidden_layer[HiddenLayerCount[id.x] - 1][j].weights[i];
        }
        activation += network.output_layer[i].bias;
        output_activations[i] = sigmoid(activation);
    }
    
    // Calculate error
    error = 0.0;
    for (int i = 0; i < OututCount[id.x]; i++) {
        error += pow(targets[i] - output_activations[i], 2);
    }
    error *= 0.5;
    
    // Backward pass
    float output_deltas[OututCount];
    float hidden_deltas[HiddenLayerCount][];

    // Calculate deltas for the output layer
    for (int i = 0; i < OututCount[id.x]; i++) {
        const float error_derivative = targets[i] - output_activations[i];
        output_deltas[i] = error_derivative * sigmoid_derivative(output_activations[i]);
    }

    // Calculate deltas for the hidden layers
    for (int i = 0; i < HiddenLayerCount[id.x]-1; i++) {
        float error_derivative = 0.0;
        for (int j = 0; j < HiddenLayerCount[id.x]; j++) {
            for (int k = 0; k < OututCount[id.x]; k++) {
                error_derivative += output_deltas[k] * network.hidden_layer[j - 1][k].weights[i];
            }
            
            hidden_deltas[i][j] = error_derivative * sigmoid_derivative(hidden_activations[i]);
        }
    }

    // Update weights and biases
    for (int i = 0; i < OututCount[id.x]; i++) {
        for (int j = 0; j < HiddenLayerCount[id.x]; j++) {
            network.output_layer[i].weights[j] += LearningRate[WaveIsFirstLane().x] * output_deltas[i] * hidden_activations[j];
        }
        network.output_layer[i].bias += LearningRate[id.x] * output_deltas[i];
    }

    for(int i = 0; i < HiddenLayerCount[id.x]+1; i++) {
        
        if(i == 0)
        {
            for (int j = 0; j < InputCount[id.x]; j++)
            {
                network.input_layer[j].weights[i] += LearningRate[id.x] * hidden_deltas[0][i] * inputs[j];
            }
        }
        else
        {
            for (int j = 0; j < HiddenLayerStructure[id.x][i]; j++) {
                network.hidden_layer[i][j].weights[i] += LearningRate[id.x] * hidden_deltas[i][j] * hidden_activations[j];
                network.hidden_layer[i][j].bias += LearningRate[id.x] * hidden_deltas[0][i];
            }
        }
    }
    
    backpropagated[id.x] = network;
}


//TODO: Fix this
// Define the inference function
void Inference(Network network, float inputs[], out float outputs[]) {
    float hidden_activations[HiddenLayerStructure];
    float output_activations[OututCount];
    
    for (int i = 0; i < HiddenLayerStructure; i++) {
        float activation = 0.0;
        for (int j = 0; j < InputCount; j++) {
            activation += inputs[j] * network.input_layer[j].weights[i];
        }
        activation += network.hidden_layer[i].bias;
        hidden_activations[i] = sigmoid(activation);
    }
    
    for (int i = 0; i < OututCount; i++) {
        float activation = 0.0;
        for (int j = 0; j < HiddenLayerStructure; j++) {
            activation += hidden_activations[j] * network.hidden_layer[j].weights[i];
        }
        activation += network.output_layer[i].bias;
        output_activations[i] = sigmoid(activation);
    }
    
    for (int i = 0; i < OututCount; i++) {
        outputs[i] = output_activations[i];
    }
    
    infered[id.x] = value;
}

void Crossover(Network network1, Network network2, out Network network3) {
    for (int i = 0; i < InputCount; i++) {
        for (int j = 0; j < HiddenLayerStructure; j++) {
            if (rand() % 2 == 0) {
                network3.input_layer[i].weights[j] = network1.input_layer[i].weights[j];
            } else {
                network3.input_layer[i].weights[j] = network2.input_layer[i].weights[j];
            }
        }
    }
    
    for (int i = 0; i < HiddenLayerStructure; i++) {
        for (int j = 0; j < OututCount; j++) {
            if (rand() % 2 == 0) {
                network3.hidden_layer[i].weights[j] = network1.hidden_layer[i].weights[j];
            } else {
                network3.hidden_layer[i].weights[j] = network2.hidden_layer[i].weights[j];
            }
        }
    }
    
    for (int i = 0; i < OututCount; i++) {
        if (rand() % 2 == 0) {
            network3.output_layer[i].bias = network1.output_layer[i].bias;
        } else {
            network3.output_layer[i].bias = network2.output_layer[i].bias;
        }
    }
    
    OutputBuffer[id.x] = ne;
}


[numthreads(64, 1, 1)] // The number of threads to execute in each thread group
void CSMain(uint3 id : SV_DispatchThreadID)
{
    // Define the network
    Network network;
    
    // Define the training data
    float inputs[4][3] = { { 0.0, 0.0, 1.0 }, { 1.0, 1.0, 1.0 }, { 1.0, 0.0, 1.0 }, { 0.0, 1.0, 1.0 } };
    float targets[4][2] = { { 0.0, 1.0 }, { 0.0, 1.0 }, { 1.0, 0.0 }, { 1.0, 0.0 } };
    
    // Train the network
    float error;
    for (int i = 0; i < 4; i++) {
        Train(network, inputs[i], targets[i], error);
    }
    
    // Infer the network
    float outputs[2];
    Infer(network, inputs[0], outputs);
    
    // Print the results
    printf("Error: %f", error);
}
